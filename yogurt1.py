import os
from pathlib import Path
import streamlit as st
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
from deep_translator import GoogleTranslator


# ================== Dil SeÃ§imi ==================
languages = {
    "TÃ¼rkÃ§e TR": "tr",
    "English GB": "en",
    "FranÃ§ais FR": "fr",
    "Deutsch DE": "de",
    "EspaÃ±ol ES": "es",
    "Ğ ÑƒÑÑĞºĞ¸Ğ¹ RU": "ru"
}

col1, col2 = st.columns([6, 4])
with col1:
    selected_lang = st.radio(
        "ğŸŒ Language:", options=list(languages.keys()), index=0, horizontal=True
    )
target_lang = languages[selected_lang]

# ================== PDF YÃ¼kleme ve VektÃ¶r DB ==================
@st.cache_data
def load_vectordb(pdf_folder="pdfs", db_path="faiss_index"):
    pdf_folder_path = Path(pdf_folder)
    if not pdf_folder_path.exists():
        st.error(f"{pdf_folder} klasÃ¶rÃ¼ bulunamadÄ±.")
        return None

    docs = []
    for pdf_file in pdf_folder_path.glob("*.pdf"):
        loader = PyPDFLoader(str(pdf_file))
        docs.extend(loader.load())

    if not docs:
        st.warning("PDF dosyasÄ± bulunamadÄ±.")
        return None

    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    vectordb = FAISS.from_documents(docs, embeddings)
    vectordb.save_local(db_path)
    st.success("VektÃ¶r veritabanÄ± oluÅŸturuldu ve kaydedildi âœ…")
    return vectordb

@st.cache_data
def load_vectordb_local(db_path="faiss_index"):
    if Path(db_path).exists():
        # embedding objesini oluÅŸtur
        embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
        return FAISS.load_local(db_path, embeddings)
    return None

# ================== HuggingFace Local Model ==================
@st.cache_resource
def get_llm_local():
    model_name = "google/flan-t5-small"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    return pipeline(
        "text2text-generation",
        model=model,
        tokenizer=tokenizer,
        max_length=512,
        temperature=0.2
    )

# ================== Prompt Template ==================
prompt_template = PromptTemplate(
    input_variables=["context", "question"],
    template="""
Sen bir ÅŸef asistanÄ±sÄ±n. AÅŸaÄŸÄ±da yoÄŸurtla ilgili tarif bilgileri iÃ§eren bir metin var:

{context}

KullanÄ±cÄ±nÄ±n verdiÄŸi malzemelere uygun, sadece yoÄŸurt iÃ§eren tarifler Ã¶ner.
TÃ¼rk mutfaÄŸÄ±na Ã¶ncelik ver. Malzeme listesi ve yapÄ±lÄ±ÅŸ adÄ±mlarÄ±nÄ± yaz.
Sade, akÄ±cÄ± ve kullanÄ±cÄ± dostu bir dille yaz. Gerekiyorsa alternatif malzemeler de Ã¶ner.

Malzemeler: {question}
"""
)

# ================== RAG Chain OluÅŸturma ==================
@st.cache_resource
def create_rag_chain(_vectordb):
    llm = get_llm_local()

    def rag_answer(query):
        # Benzer dokÃ¼manlarÄ± bul
        docs = _vectordb.similarity_search(query, k=3)
        context_text = "\n".join([doc.page_content for doc in docs])
        
        # Prompt ve dil Ã§evirisi
        input_text = prompt_template.format(context=context_text, question=query)
        if target_lang != "tr":
            input_text = GoogleTranslator(source="tr", target=target_lang).translate(input_text)

        result = llm(input_text)
        answer = result[0]["generated_text"]

        # EÄŸer hedef dil TÃ¼rkÃ§e deÄŸilse cevabÄ± tekrar Ã§evir
        if target_lang != "tr":
            answer = GoogleTranslator(source=target_lang, target="tr").translate(answer)

        return answer

    return rag_answer

# ================== Streamlit UI ==================
st.title("YoÄŸurtlu Mutfak AsistanÄ± - Offline RAG ğŸŒ")

vectordb = load_vectordb_local() or load_vectordb()

if vectordb is not None:
    rag_chain = create_rag_chain(vectordb)
    user_question = st.text_input("Malzemeleri yazÄ±nÄ±z:")
    if user_question:
        with st.spinner("Cevap hazÄ±rlanÄ±yor..."):
            answer = rag_chain(user_question)
            st.markdown(f"**Cevap:** {answer}")
else:
    st.warning("VektÃ¶r veritabanÄ± yÃ¼klenemedi.")


