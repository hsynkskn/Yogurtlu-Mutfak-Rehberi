import os
from pathlib import Path
import streamlit as st
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.embeddings import HuggingFaceEmbeddings

# ================== Dil SeÃ§imi ==================
languages = {
    "TÃ¼rkÃ§e TR": "tr",
    "English GB": "en",
    "FranÃ§ais FR": "fr",
    "Deutsch DE": "de",
    "EspaÃ±ol ES": "es",
    "Ğ ÑƒÑÑĞºĞ¸Ğ¹ RU": "ru"
}

col1, col2 = st.columns([6, 4])
with col1:
    selected_lang = st.radio(
        "ğŸŒ Language:", options=list(languages.keys()), index=0, horizontal=True
    )
target_lang = languages[selected_lang]

# ================== VektÃ¶r DB Ä°ÅŸlemleri ==================

EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"
FAISS_INDEX_PATH = "faiss_index"
PDF_FOLDER = "pdfs"

@st.cache_resource  # Use cache_resource for models/embeddings
def get_embeddings():
    """Loads and caches the embeddings model."""
    return HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)

@st.cache_data  # Use cache_data for data-like objects (like the vector DB itself)
def create_and_save_vectordb(_pdf_folder=PDF_FOLDER, _db_path=FAISS_INDEX_PATH):
    """Loads PDFs, creates FAISS index, and saves it."""
    pdf_folder_path = Path(_pdf_folder)
    if not pdf_folder_path.exists():
        st.error(f"'{_pdf_folder}' klasÃ¶rÃ¼ bulunamadÄ±. LÃ¼tfen PDF dosyalarÄ±nÄ±zÄ± buraya ekleyin.")
        return None

    docs = []
    for pdf_file in pdf_folder_path.glob("*.pdf"):
        try:
            loader = PyPDFLoader(str(pdf_file))
            docs.extend(loader.load())
        except Exception as e:
            st.warning(f"'{pdf_file.name}' dosyasÄ± yÃ¼klenirken hata oluÅŸtu: {e}")

    if not docs:
        st.warning("HiÃ§bir PDF dosyasÄ± bulunamadÄ± veya okunamadÄ±.")
        return None

    embeddings = get_embeddings()
    try:
        vectordb = FAISS.from_documents(docs, embeddings)
        vectordb.save_local(_db_path)
        st.success(f"VektÃ¶r veritabanÄ± baÅŸarÄ±yla oluÅŸturuldu ve '{_db_path}' adresine kaydedildi. âœ…")
        return vectordb
    except Exception as e:
        st.error(f"VektÃ¶r veritabanÄ± oluÅŸturulurken hata oluÅŸtu: {e}")
        return None

def load_local_vectordb(_db_path=FAISS_INDEX_PATH):
    """Loads the FAISS index from the local path."""
    embeddings = get_embeddings()
    if Path(_db_path).exists():
        try:
            return FAISS.load_local(_db_path, embeddings)
        except ValueError as e:
            st.warning(f"FAISS index yÃ¼klenirken bir hata oluÅŸtu: {e}. Yeniden oluÅŸturulmasÄ± gerekebilir.")
            return None
    return None

# ================== HuggingFace Local Model ==================
@st.cache_resource
def get_llm_local():
    """Loads and caches the local LLM pipeline."""
    model_name = "google/flan-t5-small"
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
        return pipeline(
            "text2text-generation",
            model=model,
            tokenizer=tokenizer,
            max_length=512,
            temperature=0.2
        )
    except Exception as e:
        st.error(f"LLM modeli yÃ¼klenirken hata oluÅŸtu: {e}")
        return None

# ================== RAG Chain OluÅŸturma ==================
def create_rag_chain(_vectordb):
    """Creates the RAG answering function."""
    llm = get_llm_local()
    if llm is None:
        st.error("LLM modeli yÃ¼klenemediÄŸi iÃ§in RAG zinciri oluÅŸturulamÄ±yor.")
        return None

    def rag_answer(query):
        if _vectordb is None:
            return "VeritabanÄ± mevcut deÄŸil, cevap verilemiyor."
        try:
            docs = _vectordb.similarity_search(query, k=3)
            if not docs:
                return "Ä°lgili bilgi bulunamadÄ±."
            context_text = "\n".join([doc.page_content for doc in docs])
            input_text = f"Context: {context_text}\n\nQuestion: {query}\nAnswer:"
            result = llm(input_text)
            return result[0]["generated_text"]
        except Exception as e:
            return f"Cevap Ã¼retilirken bir hata oluÅŸtu: {e}"

    return rag_answer

# ================== Streamlit UI ==================
st.title("YoÄŸurtlu Mutfak AsistanÄ± - Offline RAG ğŸŒ")

# Lokal FAISS index yÃ¼kle, yoksa PDFâ€™den oluÅŸtur
vectordb = load_local_vectordb()

if vectordb is None:
    st.warning("Yerel vektÃ¶r veritabanÄ± bulunamadÄ± veya bozuk. PDF dosyalarÄ±nÄ±zdan oluÅŸturuluyor...")
    vectordb = create_and_save_vectordb()

if vectordb is not None:
    rag_chain = create_rag_chain(vectordb)
    if rag_chain: # Check if rag_chain was created successfully
        user_question = st.text_input("AradÄ±ÄŸÄ±nÄ±z malzemeyi veya tarifi yazÄ±n:")
        if user_question:
            with st.spinner("Cevap hazÄ±rlanÄ±yor..."):
                answer = rag_chain(user_question)
                st.markdown(f"**Cevap:** {answer}")
else:
    st.warning("VektÃ¶r veritabanÄ± yÃ¼klenemedi veya oluÅŸturulamadÄ±. LÃ¼tfen PDF klasÃ¶rÃ¼nÃ¼zÃ¼ kontrol edin.")
