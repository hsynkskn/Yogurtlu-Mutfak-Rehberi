import os
from pathlib import Path
import streamlit as st
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
from deep_translator import GoogleTranslator


# ================== Dil Se√ßimi ==================
languages = {
    "T√ºrk√ße TR": "tr",
    "English GB": "en",
    "Fran√ßais FR": "fr",
    "Deutsch DE": "de",
    "Espa√±ol ES": "es",
    "–†—É—Å—Å–∫–∏–π RU": "ru"
}

col1, col2 = st.columns([6, 4])
with col1:
    selected_lang = st.radio(
        "üåê Language:", options=list(languages.keys()), index=0, horizontal=True
    )
target_lang = languages[selected_lang]

# ================== PDF Y√ºkleme ve Vekt√∂r DB ==================
def load_vectordb_local(db_path="faiss_index"):
    if os.path.exists(db_path):
        embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
        return FAISS.load_local(db_path, embeddings)
    return None

def load_vectordb(pdf_folder="pdfs", db_path="faiss_index"):
    pdf_folder_path = Path(pdf_folder)
    if not pdf_folder_path.exists():
        st.error(f"{pdf_folder} klas√∂r√º bulunamadƒ±.")
        return None

    docs = []
    for pdf_file in pdf_folder_path.glob("*.pdf"):
        loader = PyPDFLoader(str(pdf_file))
        docs.extend(loader.load())

    if not docs:
        st.warning("PDF dosyasƒ± bulunamadƒ±.")
        return None

    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    vectordb = FAISS.from_documents(docs, embeddings)
    vectordb.save_local(db_path)
    st.success("Vekt√∂r veritabanƒ± olu≈üturuldu ve kaydedildi ‚úÖ")
    return vectordb



@st.cache_resource  # <-- cache_data yerine cache_resource
def load_vectordb_local(db_path="faiss_index"):
    if os.path.exists(db_path):
        embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
        return FAISS.load_local(db_path, embeddings)
    return None

# ================== HuggingFace Local Model ==================
@st.cache_resource
def get_llm_local():
    model_name = "google/flan-t5-small"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    return pipeline(
        "text2text-generation",
        model=model,
        tokenizer=tokenizer,
        max_length=512,
        temperature=0.2
    )

# ================== Prompt Template ==================
prompt_template = PromptTemplate(
    input_variables=["context", "question"],
    template="""
Sen bir ≈üef asistanƒ±sƒ±n. A≈üaƒüƒ±da yoƒüurtla ilgili tarif bilgileri i√ßeren bir metin var:

{context}

Kullanƒ±cƒ±nƒ±n verdiƒüi malzemelere uygun, sadece yoƒüurt i√ßeren tarifler √∂ner.
T√ºrk mutfaƒüƒ±na √∂ncelik ver. Malzeme listesi ve yapƒ±lƒ±≈ü adƒ±mlarƒ±nƒ± yaz.
Sade, akƒ±cƒ± ve kullanƒ±cƒ± dostu bir dille yaz. Gerekiyorsa alternatif malzemeler de √∂ner.

Malzemeler: {question}
"""
)

# ================== RAG Chain Olu≈üturma ==================
@st.cache_resource
def create_rag_chain(_vectordb):
    llm = get_llm_local()

    def rag_answer(query):
        docs = _vectordb.similarity_search(query, k=3)
        context_text = "\n".join([doc.page_content for doc in docs])

        input_text = prompt_template.format(context=context_text, question=query)
        if target_lang != "tr":
            input_text = GoogleTranslator(source="tr", target=target_lang).translate(input_text)

        result = llm(input_text)
        answer = result[0]["generated_text"]

        if target_lang != "tr":
            answer = GoogleTranslator(source=target_lang, target="tr").translate(answer)

        return answer

    return rag_answer


# ================== Streamlit UI ==================
st.title("Yoƒüurtlu Mutfak Asistanƒ± - Offline RAG üåê")

vectordb = load_vectordb_local() or load_vectordb()
if vectordb:
    rag_chain = create_rag_chain(vectordb)
    user_question = st.text_input("Malzemeleri yazƒ±nƒ±z:")
    if user_question:
        with st.spinner("Cevap hazƒ±rlanƒ±yor..."):
            answer = rag_chain(user_question)
            st.markdown(f"**Cevap:** {answer}")
else:
    st.warning("Vekt√∂r veritabanƒ± y√ºklenemedi.")



