import os
from pathlib import Path
import streamlit as st
from groq import Groq
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.embeddings import HuggingFaceEmbeddings

# ================== Dil SeÃ§imi ==================
languages = {
    "TÃ¼rkÃ§e TR": "tr"
}

col1, col2 = st.columns([6, 4])
with col1:
    selected_lang = st.radio(
        "ğŸŒ Language:", options=list(languages.keys()), index=0, horizontal=True
    )
target_lang = languages[selected_lang]

# ================== VektÃ¶r DB ==================
EMBEDDING_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
FAISS_INDEX_PATH = "faiss_index"
PDF_FOLDER = "pdfs"
GROQ_MODEL = "llama-3.1-8b-instant"

@st.cache_resource
def get_embeddings():
    return HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)

@st.cache_data
def create_and_save_vectordb(_pdf_folder=PDF_FOLDER, _db_path=FAISS_INDEX_PATH):
    pdf_folder_path = Path(_pdf_folder)
    if not pdf_folder_path.exists():
        st.error(f"'{_pdf_folder}' klasÃ¶rÃ¼ bulunamadÄ±. LÃ¼tfen PDF dosyalarÄ±nÄ±zÄ± buraya ekleyin.")
        return None

    docs = []
    for pdf_file in pdf_folder_path.glob("*.pdf"):
        try:
            loader = PyPDFLoader(str(pdf_file))
            docs.extend(loader.load())
        except Exception as e:
            st.warning(f"'{pdf_file.name}' yÃ¼klenemedi: {e}")

    if not docs:
        st.warning("HiÃ§bir PDF bulunamadÄ±.")
        return None

    embeddings = get_embeddings()
    try:
        vectordb = FAISS.from_documents(docs, embeddings)
        vectordb.save_local(_db_path)
        st.success(f"VektÃ¶r veritabanÄ± '{_db_path}' adresine kaydedildi âœ…")
        return vectordb
    except Exception as e:
        st.error(f"FAISS oluÅŸturulamadÄ±: {e}")
        return None

def load_local_vectordb(_db_path=FAISS_INDEX_PATH):
    embeddings = get_embeddings()
    if Path(_db_path).exists():
        try:
            return FAISS.load_local(_db_path, embeddings)
        except ValueError as e:
            st.warning(f"FAISS index yÃ¼klenemedi: {e}")
            return None
    return None

# ================== Prompt TanÄ±mÄ± ==================
SYSTEM_PROMPT = """
Sen bir ÅŸef asistanÄ±sÄ±n. GÃ¶revin, aÅŸaÄŸÄ±da sunulan yoÄŸurtla ilgili tarif metinlerini ('Context') kullanarak,
kullanÄ±cÄ±nÄ±n verdiÄŸi malzemelere uygun tarifler Ã¶nermektir.

Kurallar:
1. YalnÄ±zca yoÄŸurt iÃ§eren tarifler Ã¶ner.
2. TÃ¼rk mutfaÄŸÄ±na Ã¶ncelik ver.
3. YanÄ±t, net bir 'Malzemeler' listesi ve 'YapÄ±lÄ±ÅŸ AdÄ±mlarÄ±' iÃ§ermelidir.
4. Gerekliyse, baÄŸlamda (Context) bulunmayan ancak mantÄ±klÄ± olan alternatif malzemeler de Ã¶ner.
5. Sade, akÄ±cÄ± ve kullanÄ±cÄ± dostu bir dille **TÃ¼rkÃ§e** olarak yaz.
"""
# ================== Groq API ==================
# Mevcut get_groq_client() fonksiyonu buraya kalsÄ±n...

def query_groq(prompt: str, context: str): 
    client = get_groq_client()
    if client is None:
        return "Groq API istemcisi baÅŸlatÄ±lamadÄ±."

    # Groq Chat API'si iÃ§in GeliÅŸmiÅŸ Mesaj YapÄ±sÄ±
    # 1. System Prompt: Modelin rolÃ¼nÃ¼ ve kurallarÄ±nÄ± belirler.
    # 2. User Prompt: BaÄŸlamÄ± (Context) ve KullanÄ±cÄ± Sorusunu iletir.
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": f"Context:\n{context}\n\nMalzemeler: {prompt}"}
    ]

    try:
        response = client.chat.completions.create(
            model=GROQ_MODEL, 
            messages=messages, # <-- GÃ¼ncellenmiÅŸ mesaj yapÄ±sÄ±
            temperature=0.1,
            max_tokens=512
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Groq API hatasÄ±: {e}"

# ================== RAG Chain (GÃ¼ncellenmiÅŸ) ==================
def create_rag_chain(_vectordb):
    def rag_answer(query):
        if _vectordb is None:
            return "VeritabanÄ± mevcut deÄŸil."
        try:
            # 1. Retrieval (Belge Alma)
            docs = _vectordb.similarity_search(query, k=3)
            if not docs:
                return "Ä°lgili bilgi bulunamadÄ±. LÃ¼tfen daha genel malzemelerle tekrar deneyin."
            
            context_text = "\n".join([doc.page_content for doc in docs])
            
            # 2. Generation (Sorgulama)
            # HATA DÃœZELTÄ°LDÄ°: ArtÄ±k iki parametreyi de gÃ¶nderiyoruz: (kullanÄ±cÄ± sorusu, context)
            return query_groq(query, context_text) # <-- Bu satÄ±r dÃ¼zeltildi
            
        except Exception as e:
            # Bu hata yakalama bloÄŸu, artÄ±k sizin aldÄ±ÄŸÄ±nÄ±z hatayÄ± yakalamalÄ± ve 
            # dÃ¼zgÃ¼n bir ÅŸekilde raporlamalÄ±ydÄ±, ama ÅŸimdi onu da dÃ¼zelttik.
            return f"Cevap Ã¼retilirken hata: {e}"
    return rag_answer
# ================== Streamlit UI ==================
st.title("ğŸ¥› YoÄŸurtlu Mutfak AsistanÄ± - Groq RAG")

vectordb = load_local_vectordb()
if vectordb is None:
    st.warning("Yerel FAISS index bulunamadÄ±. PDFâ€™lerden oluÅŸturuluyor...")
    vectordb = create_and_save_vectordb()

if vectordb is not None:
    rag_chain = create_rag_chain(vectordb)
    user_question = st.text_input("AradÄ±ÄŸÄ±nÄ±z malzemeyi veya tarifi yazÄ±n:")
    if user_question:
        with st.spinner("Cevap hazÄ±rlanÄ±yor (Groq API)..."):
            answer = rag_chain(user_question)
            st.markdown(f"**Cevap:** {answer}")
else:
    st.warning("VektÃ¶r veritabanÄ± yÃ¼klenemedi. LÃ¼tfen PDF klasÃ¶rÃ¼nÃ¼zÃ¼ kontrol edin.")
