import os
from pathlib import Path
import streamlit as st
from groq import Groq
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.embeddings import HuggingFaceEmbeddings

# LangChain bileÅŸenlerini ekliyoruz
from langchain.prompts import PromptTemplate 
from langchain_groq import ChatGroq
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser


# ================== Dil SeÃ§imi ==================
languages = {
    "TÃ¼rkÃ§e TR": "tr",
    "English GB": "en",
}

col1, col2 = st.columns([6, 4])
with col1:
    selected_lang = st.radio(
        "ğŸŒ Language:", options=list(languages.keys()), index=0, horizontal=True
    )
target_lang = languages[selected_lang]

# ================== VektÃ¶r DB YapÄ±landÄ±rmasÄ± ==================
EMBEDDING_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
FAISS_INDEX_PATH = "faiss_index"
PDF_FOLDER = "pdfs"
GROQ_MODEL = "llama-3.1-8b-instant" # KullanacaÄŸÄ±mÄ±z Groq modeli

@st.cache_resource
def get_embeddings():
    """HuggingFace gÃ¶mme modelini yÃ¼kler."""
    return HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)

@st.cache_data
def create_and_save_vectordb(_pdf_folder=PDF_FOLDER, _db_path=FAISS_INDEX_PATH):
    """PDF'leri yÃ¼kler, parÃ§alar, gÃ¶mer ve FAISS veritabanÄ± oluÅŸturup kaydeder."""
    pdf_folder_path = Path(_pdf_folder)
    if not pdf_folder_path.exists():
        st.error(f"'{_pdf_folder}' klasÃ¶rÃ¼ bulunamadÄ±. LÃ¼tfen PDF dosyalarÄ±nÄ±zÄ± buraya ekleyin.")
        return None

    docs = []
    for pdf_file in pdf_folder_path.glob("*.pdf"):
        try:
            # Sadece PyPDFLoader deÄŸil, tÃ¼m dosya yÃ¼kleyicilerini desteklemek iÃ§in (Gerekliyse)
            loader = PyPDFLoader(str(pdf_file))
            docs.extend(loader.load())
        except Exception as e:
            st.warning(f"'{pdf_file.name}' yÃ¼klenemedi: {e}")

    if not docs:
        st.warning("HiÃ§bir PDF bulunamadÄ±.")
        return None

    embeddings = get_embeddings()
    try:
        # FAISS veritabanÄ±nÄ± oluÅŸturma
        vectordb = FAISS.from_documents(docs, embeddings)
        vectordb.save_local(_db_path)
        st.success(f"VektÃ¶r veritabanÄ± '{_db_path}' adresine kaydedildi âœ…")
        return vectordb
    except Exception as e:
        st.error(f"FAISS oluÅŸturulamadÄ±: {e}")
        return None

def load_local_vectordb(_db_path=FAISS_INDEX_PATH):
    """Yerel olarak kaydedilmiÅŸ FAISS veritabanÄ±nÄ± yÃ¼kler."""
    embeddings = get_embeddings()
    if Path(_db_path).exists():
        try:
            return FAISS.load_local(_db_path, embeddings)
        except ValueError as e:
            # EÄŸer FAISS index'in formatÄ± deÄŸiÅŸmiÅŸse bu hatayÄ± alabiliriz.
            st.warning(f"FAISS index yÃ¼klenemedi: {e}. Yeniden oluÅŸturmayÄ± deneyin.")
            return None
    return None

# ================== Groq API ve Model ==================
def get_groq_llm():
    """LangChain iÃ§in Groq Chat Modelini dÃ¶ndÃ¼rÃ¼r."""
    api_key = os.getenv("GROQ_API_KEY")
    if not api_key:
        st.error("âŒ GROQ_API_KEY ortam deÄŸiÅŸkeni bulunamadÄ±. LÃ¼tfen ayarla.")
        return None
    
    # LangChain-Groq entegrasyonu, doÄŸrudan os.getenv() iÃ§indeki anahtarÄ± kullanÄ±r.
    llm = ChatGroq(
        model=GROQ_MODEL,
        temperature=0.2,
        max_tokens=512
    )
    return llm

# ================== Prompt TanÄ±mÄ± ==================
# Ä°stediÄŸiniz PromptTemplate
prompt_template = PromptTemplate(
    input_variables=["context", "question"],
    template="""
Sen bir ÅŸef asistanÄ±sÄ±n. AÅŸaÄŸÄ±da yoÄŸurtla ilgili tarif bilgileri iÃ§eren bir metin var:

{context}

KullanÄ±cÄ±nÄ±n verdiÄŸi malzemelere uygun, sadece yoÄŸurt iÃ§eren tarifler Ã¶ner.
TÃ¼rk mutfaÄŸÄ±na Ã¶ncelik ver. Malzeme listesi ve yapÄ±lÄ±ÅŸ adÄ±mlarÄ±nÄ± yaz.
Sade, akÄ±cÄ± ve kullanÄ±cÄ± dostu bir dille yaz. Gerekiyorsa alternatif malzemeler de Ã¶ner.

Malzemeler: {question}
"""
You're a chef's assistant. Below is a text containing yogurt recipe information:

{context}

Suggest yogurt-only recipes based on the ingredients provided by the user.
Prioritize Turkish cuisine. Provide a list of ingredients and instructions.
Write in simple, fluent, and user-friendly language. Suggest alternative ingredients if necessary.

Ingredients: {question}
"""
)

# ================== RAG Zinciri (LCEL) ==================
def create_rag_chain_lcel(_vectordb):
    """LCEL kullanarak RAG zincirini oluÅŸturur."""
    llm = get_groq_llm()
    if llm is None:
        return None

    # Retriever (FAISS veritabanÄ±ndan belge alÄ±cÄ±)
    retriever = _vectordb.as_retriever(search_kwargs={"k": 3})

    # LCEL Zinciri:
    # 1. RunnablePassthrough: KullanÄ±cÄ±nÄ±n sorusunu alÄ±r.
    # 2. 'context' kÄ±smÄ±: Soru ile ilgili belgeleri (docs) alÄ±r, string'e Ã§evirir.
    # 3. 'question' kÄ±smÄ±: KullanÄ±cÄ±nÄ±n orijinal sorusunu korur.
    # 4. Prompt: 'context' ve 'question' ile prompt'u hazÄ±rlar.
    # 5. LLM: HazÄ±rlanan prompt'u Groq modeline gÃ¶nderir.
    # 6. StrOutputParser: Modelin Ã§Ä±ktÄ±sÄ±nÄ± temiz bir string'e Ã§evirir.
    rag_chain = (
        {"context": retriever | (lambda docs: "\n".join([doc.page_content for doc in docs])), 
         "question": RunnablePassthrough()
        }
        | prompt_template
        | llm
        | StrOutputParser()
    )
    return rag_chain


# ================== Streamlit UI ==================
st.title("ğŸ¥› YoÄŸurtlu Mutfak AsistanÄ±")

# VektÃ¶r veritabanÄ±nÄ± yÃ¼kle veya oluÅŸtur
vectordb = load_local_vectordb()
if vectordb is None:
    st.info("Yerel FAISS index bulunamadÄ±. PDFâ€™lerden oluÅŸturuluyor...")
    vectordb = create_and_save_vectordb()

# Sorgulama arayÃ¼zÃ¼
if vectordb is not None:
    # RAG Zincirini oluÅŸtur
    rag_chain = create_rag_chain_lcel(vectordb)
    
    if rag_chain is not None:
        user_question = st.text_input("AradÄ±ÄŸÄ±nÄ±z malzemeyi veya tarifi yazÄ±n:")
        
        if user_question:
            with st.spinner("Cevap hazÄ±rlanÄ±yor (Groq API)..."):
                # Zinciri Ã§alÄ±ÅŸtÄ±rma
                answer = rag_chain.invoke(user_question)
                st.markdown(f"**Cevap:** {answer}")
    else:
        st.error("RAG zinciri baÅŸlatÄ±lamadÄ± (GROQ_API_KEY eksik olabilir).")
else:
    st.warning("VektÃ¶r veritabanÄ± yÃ¼klenemedi. LÃ¼tfen PDF klasÃ¶rÃ¼nÃ¼zÃ¼ kontrol edin.")
